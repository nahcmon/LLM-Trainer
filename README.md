# LLM Training Web Interface

A comprehensive web-based interface for training GPT-style language models from scratch with real-time monitoring and configuration.

## Quick Start

### 1. Setup (First Time Only)

Run the setup script to install all dependencies:

```bash
setup.bat
```

This will:
- Create a virtual environment
- Install all required packages (PyTorch, Transformers, FastAPI, etc.)
- Install FlashAttention-2 for 5-10x faster training
- Set up the web application

### 2. Start the Web App

```bash
start.bat
```

Then open your browser to: **http://localhost:2345**

## Features

### ğŸ¨ **Comprehensive Configuration**
- **40+ training parameters** organized into 8 intuitive categories
- **Interactive tooltips** (?) explaining every parameter with typical values
- **Model presets** from 10M to 70B parameters (GPT-2, GPT-3, LLaMA sizes)
- **Real-time VRAM estimation** as you adjust parameters
- **Multiple dataset support** with interleaving or concatenation
- **Advanced optimizer settings** (AdamW with Î²1, Î²2, Îµ configuration)
- **Learning rate schedulers** (linear, cosine, constant with warmup)
- **Precision options** (FP16, BF16, FP32) with gradient checkpointing

### ğŸ“Š **Live Monitoring**
- **Real-time loss tracking** with step-by-step updates
- **Training progress bar** showing completion percentage
- **GPU metrics** (utilization, memory, temperature)
- **Elapsed time** and estimated time remaining
- **Color-coded log streaming** with auto-scroll
- **WebSocket-based updates** for instant feedback (no polling)

### ğŸ›ï¸ **Easy Control**
- **One-click training** start/stop controls
- **Auto-saving checkpoints** at configurable intervals
- **Resume from checkpoints** to continue interrupted training
- **Background training** - close browser, training continues
- **Graceful shutdown** with checkpoint saving on stop

### ğŸ”§ **Advanced Features**
- **FlashAttention-2 support** for 5-10x faster attention computation
- **Gradient checkpointing** to train larger models with less VRAM
- **Gradient accumulation** to simulate large batch sizes
- **Gradient clipping** to prevent training instability
- **Mixed precision training** with automatic loss scaling
- **Multi-dataset training** - combine datasets with custom mixing
- **Model export** to SafeTensors, GGUF (llama.cpp), or PyTorch formats
- **SentencePiece tokenizer** trained on your dataset
- **Automatic fallbacks** (SDPA â†’ manual attention if FlashAttention unavailable)

## Web Interface Overview

### Left Panel: Configuration (8 Categories)
- **ğŸ“š Dataset** - Choose HuggingFace datasets, combine multiple datasets, configure mixing
- **ğŸ—ï¸ Model Architecture** - Layers, hidden size, attention heads, FFN inner dimension
- **âš™ï¸ Model Hyperparameters** - Dropout rates, activation functions, layer norm epsilon
- **ğŸ¯ Training** - Batch size, epochs, gradient accumulation, max steps
- **ğŸ“ˆ Optimizer** - Learning rate, weight decay, AdamW beta parameters, epsilon
- **ğŸ“Š Scheduler** - LR scheduling (linear/cosine/constant), warmup steps, decay
- **âš¡ Precision** - FP16/BF16/FP32, gradient checkpointing, gradient clipping
- **ğŸ’¾ Logging** - Checkpoint intervals, logging frequency, output directory

### Right Panel: Monitoring
- **Progress Stats** - Current step, epoch, loss, learning rate, GPU stats
- **Progress Bar** - Visual training completion with percentage
- **Training Logs** - Real-time color-coded log streaming with auto-scroll
- **VRAM Estimate** - Before training starts, see estimated memory usage

## Configuration Tips

### Quick Training Test (< 1 minute)
1. Select model preset: "tiny-10M" (10 million parameters)
2. Set `epochs` to 1
3. Set `logging_steps` to 10
4. Keep default dataset (WikiText)
5. Click "Start Training"

### Memory-Efficient Settings (for 8GB VRAM)
- Enable `gradient_checkpointing` âœ“
- Use `fp16` precision
- Use gradient accumulation (set `gradient_accumulation_steps` to 4)
- Keep `seq_length` â‰¤ 1024
- Choose model preset â‰¤ 500M parameters

### Performance Optimization (for speed)
- Install FlashAttention-2 (automatic in setup.bat)
- Use `fp16` or `bf16` precision (2x faster than fp32)
- Disable gradient checkpointing if VRAM allows
- Increase `batch_size` or `gradient_accumulation_steps`
- Close other GPU applications

### Model Size Selection
Use the **Model Preset** dropdown:
- **tiny-10M / tiny-50M**: Testing and debugging (< 1GB VRAM)
- **small-100M**: Quick experiments, similar to GPT-2 Small (~3GB VRAM)
- **medium-350M**: GPT-2 Medium size (~5GB VRAM)
- **medium-500M**: Balanced quality/speed (~7GB VRAM)
- **large-1B+**: High quality models (12GB+ VRAM required)

### Hover Over "?" Icons
Every parameter has a tooltip explaining:
- What it does
- Typical values
- When to adjust it
- Impact on training and memory

## Troubleshooting

### Server Issues

**"Server stopped/crashed immediately"**
- This is NORMAL! The server is actually running.
- When you see "Application startup complete", the server is ready.
- The command window stays open waiting for requests - don't close it!
- Open your browser to http://localhost:2345 to see the interface.
- To verify: run `test_server.bat` in another terminal

**Port Already in Use**
- Change the port in `web_app.py` (line with `port=2345`)
- Or close the other application using port 2345

**"pynvml deprecated" warning**
- This warning is harmless and will be gone after running the new `setup.bat`
- The new version uses `nvidia-ml-py` instead

### GPU/CUDA Issues

**"PyTorch CPU version detected"**
- Re-run `setup.bat` and choose option 1 (GPU/CUDA)
- Make sure you have NVIDIA GPU drivers installed
- Run `nvidia-smi` in command prompt to verify GPU is detected

**CUDA Out of Memory**
- Reduce `batch_size`
- Enable `gradient_checkpointing`
- Reduce `seq_length`
- Reduce `hidden_size` or `n_layers`

**FlashAttention Not Available**
- The trainer will automatically fall back to standard attention
- Performance may be slower but training will work
- To install FlashAttention: re-run `setup.bat` and choose GPU option
- Requires: NVIDIA GPU, CUDA drivers, Visual Studio Build Tools

### Testing Your Installation

Run this to verify everything is working:
```bash
test_server.bat
```

This will check:
- âœ“ Python environment
- âœ“ PyTorch and CUDA
- âœ“ FlashAttention availability
- âœ“ Server connectivity

## File Structure

```
llm_trainer/
â”œâ”€â”€ web_app.py                     # FastAPI application entry point (port 2345)
â”œâ”€â”€ main.py                        # CLI entry point (legacy)
â”œâ”€â”€ trainer.py                     # Core training loop with callbacks
â”œâ”€â”€ model.py                       # GPT-2 with FlashAttention/SDPA
â”œâ”€â”€ model_export.py                # Export to SafeTensors/GGUF/PyTorch
â”œâ”€â”€ dataset.py                     # HuggingFace dataset loader (multi-dataset support)
â”œâ”€â”€ tokenizer.py                   # SentencePiece tokenizer training
â”œâ”€â”€ gpu_utils.py                   # GPU monitoring and VRAM estimation
â”œâ”€â”€ setup.bat                      # One-time setup script
â”œâ”€â”€ start.bat                      # Start the web server
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ api/                           # FastAPI backend
â”‚   â”œâ”€â”€ routes.py                  # REST API endpoints
â”‚   â”œâ”€â”€ websocket.py               # WebSocket handler for real-time updates
â”‚   â””â”€â”€ models.py                  # Pydantic models for API
â”œâ”€â”€ core/                          # Core training logic
â”‚   â””â”€â”€ training_manager.py        # Singleton managing training state
â”œâ”€â”€ utils/                         # Utilities
â”‚   â”œâ”€â”€ parameter_definitions.py   # 40+ parameter definitions with tooltips
â”‚   â”œâ”€â”€ model_presets.py           # Model size presets (10M - 70B)
â”‚   â””â”€â”€ resource_estimator.py      # VRAM and compute estimation
â”œâ”€â”€ static/                        # Frontend (vanilla JS)
â”‚   â”œâ”€â”€ index.html                 # Main UI
â”‚   â”œâ”€â”€ css/styles.css             # Styling
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ main.js                # UI initialization
â”‚       â”œâ”€â”€ config-form.js         # Dynamic form generation
â”‚       â”œâ”€â”€ websocket-client.js    # WebSocket client
â”‚       â”œâ”€â”€ tooltips.js            # Tooltip system
â”‚       â”œâ”€â”€ model-presets.js       # Model preset dropdown
â”‚       â””â”€â”€ resource-estimator.js  # VRAM estimation UI
â””â”€â”€ output/                        # Training outputs (created on first run)
    â”œâ”€â”€ checkpoints/               # Model checkpoints
    â””â”€â”€ logs/                      # Training logs
```

## Technical Stack

**Backend:**
- PyTorch 2.1+ with CUDA 12.1
- Transformers (HuggingFace) for GPT-2 architecture
- FastAPI + Uvicorn for web server
- WebSockets for real-time updates
- SentencePiece for tokenization

**Frontend:**
- Vanilla JavaScript (no frameworks)
- WebSocket client for live updates
- Responsive CSS with dark theme

**Optimization:**
- FlashAttention-2 (5-10x faster attention)
- Gradient checkpointing (2-3x VRAM reduction)
- Mixed precision training (FP16/BF16)
- PyTorch SDPA fallback (if FlashAttention unavailable)

## CLI Usage (Legacy)

If you prefer the command line:

```bash
python main.py
```

Edit `main.py` to configure training parameters. The CLI version uses the same trainer but without the web interface.

## System Requirements

**Minimum:**
- Python 3.8+
- 8GB RAM
- 10GB disk space for dependencies
- NVIDIA GPU with 4GB+ VRAM (or CPU, much slower)

**Recommended:**
- Python 3.10+
- 16GB+ RAM
- NVIDIA GPU with 8GB+ VRAM (RTX 3060 or better)
- CUDA 11.8+ drivers

**Optimal:**
- Python 3.11
- 32GB+ RAM
- NVIDIA GPU with 24GB+ VRAM (RTX 3090, RTX 4090, A100)
- CUDA 12.1+ drivers

## Export Formats

After training, export your model in multiple formats:

- **SafeTensors** (default) - HuggingFace standard, fastest loading
- **GGUF** - For llama.cpp inference (CPU/GPU)
- **PyTorch** - Standard .pth checkpoint

Configure export format in the web UI or via `export_format` parameter.

## License

MIT License - Feel free to use and modify!
